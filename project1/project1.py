import sys
import scipy.special
import numpy as np
import networkx as nx
import csv
from dash import Dash, html
import dash_cytoscape as cyto
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm


def inneighbors(G, i):
    """Helper function for finding the parents of a variable."""
    return list(G.predecessors(i))

def prior_shape(vars, G):
    """Algorithm 4.2 - Page 81 of the book.
    that this function returns takes the
    same form as the statistics generated by algorithm 4.1. To determine
    the appropriate dimensions, the
    function takes as input the list of
    variables vars and structure G."""

    n = len(vars)
    r = [vars[i].r for i in range(n)]
    q = np.array([np.prod(np.array([r[j] for j in inneighbors(G,i)])) for i in range(n)], dtype=int)
    return [(q[i], r[i]) for i in range(n)]

def prior_shape_for_single_var(vars, G, var_index):
    q_var = np.prod(np.array([vars[j].r for j in inneighbors(G,var_index)]))
    return (q_var, vars[var_index].r)

def sub2ind(siz, x):
    """Algorithm 4.1. - Page 75 of the book - Helper function."""
    return np.ravel_multi_index(x, siz)

def statistics(vars, G, df):
    """Algorithm 4.1. - Page 75 of the book.
    A function for extracting the statistics, or counts,
    from a discrete data set D, assuming a Bayesian network with variables vars and structure G. The
    data set is an n x m matrix, where
    n is the number of variables and
    m is the number of data points.
    This function returns an array M of
    length n. The ith component consists of a qi x ri matrix of counts.
    The sub2ind(siz, x) function returns a linear index into an array
    with dimensions specified by siz
    given coordinates x. It is used to
    identify which parental instantiation is relevant to a particular data
    point and variable."""

    n = len(vars)
    r = np.array([vars[i].r for i in range(n)])
    q = np.array([np.prod(np.array([r[j] for j in inneighbors(G,i)])) for i in range(n)], dtype=int)
    M = [np.zeros((q[i], r[i])) for i in range(n)]
    
    for var_index in range(n):
        parents = inneighbors(G,var_index)
        r_parents = np.array([vars[j].r for j in parents])
        has_no_parent = len(parents) == 0
        df2 = df.groupby(by=[vars[i].name for i in [var_index] + (parents)])['count'].sum().reset_index()
        for index, row in df2.iterrows():
            k = row[0] - 1 # value of variable
            j = 0 if has_no_parent else sub2ind(r_parents, row[1:-1] - 1)
            M[var_index][j,k] += row[-1]
    return M

def statistics_for_single_var(vars, G, df, var_index):
    """Computes M for a single var_index.
    This version is optimized for speed.
    It groups the data by parents and data instantiation to reduce massively the number of iterations."""

    q_var = np.prod(np.array([vars[j].r for j in inneighbors(G,var_index)], dtype=int))
    M_var = np.zeros((q_var, vars[var_index].r))
    parents = inneighbors(G,var_index)
    r_parents = np.array([vars[j].r for j in parents])
    has_no_parent = len(parents) == 0
    df2 = df.groupby(by=[vars[i].name for i in [var_index] + (parents)])['count'].sum().reset_index()
    # A row in df2 is now: [var, parent1, parent2, ..., count]
    # Which also simplifies the slicing

    for index, row in df2.iterrows():
        k = row[0] - 1 # value of variable
        j = 0 if has_no_parent else sub2ind(r_parents, row[1:-1] - 1)
        M_var[j,k] += row[-1]
    return M_var

def bayesian_score_component(M, alpha_shape):
    """Algorithm 5.1 - Page 98 of the book - Helper function."""
    # I've optimized the next line by using the fact that alpha is a vector of 1s
    # p = np.sum(scipy.special.loggamma(alpha + M))
    p = np.sum(scipy.special.loggamma(1 + M))

    # I've removed the next line because with a prior of 1, the loggamma of alpha is 0
    # p -= np.sum(scipy.special.loggamma(alpha))

    # The next line has been removed to be optimized by what follows (using the fact that alpha is a vector of 1s)
    # p += np.sum(scipy.special.loggamma(np.sum(alpha, axis=1)))
    p += alpha_shape[0] * np.log(scipy.special.factorial(alpha_shape[1] - 1))

    # I've optimized the next line by using the fact that alpha is a vector of 1s
    # p -= np.sum(scipy.special.loggamma(np.sum(alpha, axis=1) + np.sum(M, axis=1)))
    p -= np.sum(scipy.special.loggamma(alpha_shape[1] + np.sum(M, axis=1)))
    return p


def bayesian_score(vars, G, D):
    """Algorithm 5.1 - Page 98 of the book.
    for computing the Bayesian score
    for a list of variables vars and
    a graph G given data D. This
    method uses a uniform prior
    αijk = 1 for all i, j, and k
    as generated by algorithm 4.2.
    The loggamma function is provided
    by SpecialFunctions.jl. Chapter 4 introduced the statistics
    and prior functions. Note that
    log(Γ(α)/Γ(α + m)) = log Γ(α) −
    log Γ(α + m), and that log Γ(1) =
    0."""

    n = len(vars)
    M = statistics(vars, G, D)
    alpha_shape = prior_shape(vars, G)
    score_components = np.array([bayesian_score_component(M[i], alpha_shape[i]) for i in range(n)])
    score = np.sum(score_components)
    return score, score_components

def bayesian_score_recompute_single_var(previous_score, previous_score_components, vars, G, D, var_index):
    """Recomputes the bayesian score form a previous score after adding a node from i to j."""
    M_j = statistics_for_single_var(vars, G, D, var_index)
    alpha_shape_j = prior_shape_for_single_var(vars, G, var_index)
    new_score_component = bayesian_score_component(M_j, alpha_shape_j)
    score = previous_score + new_score_component - previous_score_components[var_index]
    return score, new_score_component

def write_gph(dag, idx2names, filename):
    with open(filename, 'w') as f:
        for edge in dag.edges():
            f.write("{}, {}\n".format(idx2names[edge[0]], idx2names[edge[1]]))

class Variable:
    def __init__(self, name, r):
        self.name = name
        self.r = r

def compute(infile, outfile):
    df = pd.read_csv(infile, delimiter=',')
    df_max = df.max()
    var_names = list(df.columns)
    df = df.groupby(var_names).size().reset_index(name='count')
    vars = [Variable(var_names[i], df_max[i]) for i in range(len(var_names))]
    
    # JUST FOR TESTING
    # G = nx.DiGraph()
    # for i in range(len(vars)): G.add_node(i)
    # for i in range(len(vars)//2): G.add_edge(2*i, 2*i+1)

    # NUM_ITER_TIMEIT = 100
    # from time import time

    # initial_score, init_comp = bayesian_score(vars, G, df)
    # print("Initial Bayesian score: {}".format(initial_score))
    # print("Initial Bayesian score components: {}".format(init_comp))

    # G.add_edge(0, 2)
    # t_start = time()
    # for _ in tqdm(range(NUM_ITER_TIMEIT)):
    #     new_score, new_comp = bayesian_score(vars, G, df)
    # t_end = time()
    # print("\nNew Bayesian score: {}".format(new_score))
    # print("New Bayesian score components: {}".format(new_comp))
    # print("Time taken for {} iterations: {} s".format(NUM_ITER_TIMEIT, round(t_end - t_start, 2)))

    # t_start = time()
    # for _ in tqdm(range(NUM_ITER_TIMEIT)):
    #     clever_score, clever_comp = bayesian_score_recompute_single_var(initial_score, init_comp, vars, G, df, 2)
    # t_end = time()
    # clever_compoments = init_comp.copy()
    # clever_compoments[2] = clever_comp
    # print("\nClever Bayesian score: {}".format(clever_score))
    # print("Clever Bayesian score components: {}".format(clever_compoments))
    # print("Time taken for {} iterations: {} s".format(NUM_ITER_TIMEIT, round(t_end - t_start, 2)))

    # k2_iter(vars, df, 1000, max_parents=4, name="large")
    local_search_with_optis(vars, df, k_max=100000, name="large_annealing")


def k2_iter(vars, df, num_iter, max_parents=2, name="small"):
    #past_orderings = set()
    best_score = -np.inf
    best_G = None

    # Compute empty score
    G = nx.DiGraph()
    G.add_nodes_from(list(range(len(vars))))
    empty_score, empty_score_comp = bayesian_score(vars, G, df)

    # idx2names
    idx2names = {i: vars[i].name for i in range(len(vars))}

    for idx in tqdm(range(num_iter)):
        # generate a random ordering
        ordering = np.random.permutation(len(vars))
        #while ordering in past_orderings:
        #    ordering = np.random.permutation(len(vars))
        #past_orderings.add(ordering)

        # run k2 on the ordering
        G, score = k2(ordering, vars, df, max_parents=max_parents, empty_score=empty_score, empty_score_comp=empty_score_comp.copy())
        if score > best_score:
            best_score = score
            best_G = G
            write_gph(best_G, idx2names, "results/best_" + name + "_" + str(idx) + ".gph")
            print("New best score: {}".format(best_score))
    return best_G, best_score


# K2 algorithm
def k2(ordering, vars, df, max_parents=2, empty_score=None, empty_score_comp=None):
    G = nx.DiGraph()
    G.add_nodes_from(list(range(len(ordering))))
    score, score_comp = empty_score, empty_score_comp
    if score is None or score_comp is None: score, score_comp = bayesian_score(vars, G, df)
    for (k, i) in enumerate(ordering[1:]):
        if len(inneighbors(G, i)) >= max_parents:
            continue
        while True:
            score_best, j_best, score_comp_best = -np.inf, 0, None
            for j in ordering[:k]:
                if not G.has_edge(j, i):
                    G.add_edge(j, i)
                    new_score, new_score_comp = bayesian_score_recompute_single_var(score, score_comp, vars, G, df, i)
                    if new_score > score_best:
                        score_best, j_best, score_comp_best = new_score, j, new_score_comp
                    G.remove_edge(j, i)
            if score_best > score:
                score = score_best
                score_comp[i] = score_comp_best
                G.add_edge(j_best, i)
            else:
                break
    return G, score_best



def is_cyclic(G):
    return nx.is_directed_acyclic_graph(G) == False

def rand_graph_neighbor_with_score(G, score, score_comp, df, vars):
    n = G.number_of_nodes()
    i = np.random.randint(1, n)
    j = i
    while j == i:
        j = np.random.randint(1, n)
    G_prime = G.copy()
    if G.has_edge(i, j):
        G_prime.remove_edge(i, j)
    else:
        G_prime.add_edge(i, j)
    if is_cyclic(G_prime):
        return G_prime, None, None, j
    score_prime, score_comp_prime = bayesian_score_recompute_single_var(score, score_comp, vars, G_prime, df, j)
    return G_prime, score_prime, score_comp_prime, j

# Local Search algorithm
def local_search(vars, df, k_max, name):
    # Generate initial graph
    G = nx.DiGraph()
    G.add_nodes_from(list(range(len(vars))))
    score, score_comp = bayesian_score(vars, G, df)
    idx2names = {i: vars[i].name for i in range(len(vars))}

    for k in tqdm(range(k_max)):
        G_prime, score_prime, score_comp_prime, j = rand_graph_neighbor_with_score(G, score, score_comp, df, vars)
        if is_cyclic(G_prime):
            continue
        if score_prime > score:
            score = score_prime
            score_comp[j] = score_comp_prime    
            G = G_prime
            print("New best score: {}".format(score))
            if score > -425000: write_gph(G, idx2names, "results/best_" + name + "_" + str(k) + ".gph")


# Local Search algorithm with Simulated annealing, random restarts and random initializations
def random_graph_init(vars, df):
    G = nx.DiGraph()
    G.add_nodes_from(list(range(len(vars))))
    score, score_comp = bayesian_score(vars, G, df)
    return G, score, score_comp

def local_search_with_optis(vars, df, k_max, name,
                            t_max=40,
                            k_max_without_improvements=2000,
                            score_improvement_to_save=1000,
                            score_min_to_save=-425000,
                            log_score_every=1000):
    # Generate initial graph
    G, score, score_comp = random_graph_init(vars, df)
    idx2names = {i: vars[i].name for i in range(len(vars))}

    # To keep track of the best graph
    last_saved_score = -np.inf
    k_last_improvement = -1
    k_last_restart = 0

    for k in tqdm(range(k_max)):
        temp = t_max * (1 - (k - k_last_restart) / (k_max - k_last_restart) )
        G_prime, score_prime, score_comp_prime, j = rand_graph_neighbor_with_score(G, score, score_comp, df, vars)
        if score_prime is None: continue # This means that the graph is cyclic

        # Simulated annealing
        diff = score_prime - score
        if diff > 0 or np.random.rand() < np.exp(diff/temp):
            score = score_prime
            score_comp[j] = score_comp_prime    
            G = G_prime
        
        # Random restarts
        if diff > 0:
            k_last_improvement = k
        if k - k_last_improvement > k_max_without_improvements:
            k_last_restart = k
            G, score, score_comp = random_graph_init(vars, df)

        # Saving best graph
        if diff > 0 and score >= score_min_to_save and score >= last_saved_score + score_improvement_to_save:
            last_saved_score = score
            print("New best score: {}".format(score))
            write_gph(G, idx2names, "results/best_" + name + "_" + str(int(round(-score))) + ".gph")

        # Logging
        if k % log_score_every == 0:
            print("Current score: {}".format(score))


def main():
    if len(sys.argv) != 3:
        raise Exception("usage: python project1.py <infile>.csv <outfile>.gph")

    inputfilename = sys.argv[1]
    outputfilename = sys.argv[2]
    compute(inputfilename, outputfilename)


if __name__ == '__main__':
    main()
