import sys
import scipy.special
import numpy as np
import networkx as nx
import csv
from dash import Dash, html
import dash_cytoscape as cyto
import matplotlib.pyplot as plt
import pandas as pd


def inneighbors(G, i):
    """Helper function for finding the parents of a variable."""
    return list(G.predecessors(i))

def prior(vars, G):
    """Algorithm 4.2 - Page 81 of the book.
    that this function returns takes the
    same form as the statistics generated by algorithm 4.1. To determine
    the appropriate dimensions, the
    function takes as input the list of
    variables vars and structure G."""

    n = len(vars)
    r = [vars[i].r for i in range(n)]
    q = np.array([np.prod(np.array([r[j] for j in inneighbors(G,i)])) for i in range(n)], dtype=int)
    return [np.ones((q[i], r[i])) for i in range(n)]

def prior_for_single_var(vars, G, var_index):
    q_var = np.prod(np.array([vars[j].r for j in inneighbors(G,var_index)]))
    return np.ones((q_var, vars[var_index].r))

def sub2ind(siz, x):
    """Algorithm 4.1. - Page 75 of the book - Helper function."""
    return np.ravel_multi_index(x, siz)

def statistics(vars, G, D):
    """Algorithm 4.1. - Page 75 of the book.
    A function for extracting the statistics, or counts,
    from a discrete data set D, assuming a Bayesian network with variables vars and structure G. The
    data set is an n x m matrix, where
    n is the number of variables and
    m is the number of data points.
    This function returns an array M of
    length n. The ith component consists of a qi x ri matrix of counts.
    The sub2ind(siz, x) function returns a linear index into an array
    with dimensions specified by siz
    given coordinates x. It is used to
    identify which parental instantiation is relevant to a particular data
    point and variable."""

    n = len(vars)
    r = np.array([vars[i].r for i in range(n)])
    q = np.array([np.prod(np.array([r[j] for j in inneighbors(G,i)])) for i in range(n)], dtype=int)
    M = [np.zeros((q[i], r[i])) for i in range(n)]
    for index, row in D.iterrows():
        row = np.array(row)
        for i in range(n):
            k = row[i] - 1 # value of variable i
            parents = inneighbors(G,i)
            j = 0
            if len(parents) > 0: # if i has parents
                j = sub2ind(r[parents], row[parents] - 1)
            M[i][j,k] += 1
    return M

def statistics_for_single_var(vars, G, D, var_index):
    """Algorithm 4.1. - Page 75 of the book.
    A function for extracting the statistics, or counts,
    from a discrete data set D, assuming a Bayesian network with variables vars and structure G. The
    data set is an n x m matrix, where
    n is the number of variables and
    m is the number of data points.
    This function returns an array M of
    length n. The ith component consists of a qi x ri matrix of counts.
    The sub2ind(siz, x) function returns a linear index into an array
    with dimensions specified by siz
    given coordinates x. It is used to
    identify which parental instantiation is relevant to a particular data
    point and variable."""

    q_var = np.prod(np.array([vars[j].r for j in inneighbors(G,var_index)]))
    M_var = np.zeros((q_var, vars[var_index].r))
    parents = inneighbors(G,var_index)
    r_parents = np.array([vars[j].r for j in parents])
    for index, row in D.iterrows():
        row = np.array(row)
        k = row[var_index] - 1 # value of variable
        j = 0
        if len(parents) > 0: # if i has parents
            j = sub2ind(r_parents, row[parents] - 1)
        M_var[j,k] += 1
    return M_var

def bayesian_score_component(M, alpha):
    """Algorithm 5.1 - Page 98 of the book - Helper function."""
    p = np.sum(scipy.special.loggamma(alpha + M))
    p -= np.sum(scipy.special.loggamma(alpha))
    p += np.sum(scipy.special.loggamma(np.sum(alpha, axis=1)))
    p -= np.sum(scipy.special.loggamma(np.sum(alpha, axis=1) + np.sum(M, axis=1)))
    return p

def bayesian_score(vars, G, D):
    """Algorithm 5.1 - Page 98 of the book.
    for computing the Bayesian score
    for a list of variables vars and
    a graph G given data D. This
    method uses a uniform prior
    αijk = 1 for all i, j, and k
    as generated by algorithm 4.2.
    The loggamma function is provided
    by SpecialFunctions.jl. Chapter 4 introduced the statistics
    and prior functions. Note that
    log(Γ(α)/Γ(α + m)) = log Γ(α) −
    log Γ(α + m), and that log Γ(1) =
    0."""

    n = len(vars)
    M = statistics(vars, G, D)
    alpha = prior(vars, G)
    score_components = np.array([bayesian_score_component(M[i], alpha[i]) for i in range(n)])
    score = np.sum(score_components)
    return score, score_components

def bayesian_score_recompute_single_var(previous_score, previous_score_components, vars, G, D, var_index):
    """Recomputes the bayesian score form a previous score after adding a node from i to j."""
    M_j = statistics_for_single_var(vars, G, D, var_index)
    alpha_j = prior_for_single_var(vars, G, var_index)
    new_score_component = bayesian_score_component(M_j, alpha_j)
    score = previous_score + new_score_component - previous_score_components[var_index]
    new_score_components = previous_score_components.copy()
    new_score_components[var_index] = new_score_component
    return score, new_score_components

def write_gph(dag, idx2names, filename):
    with open(filename, 'w') as f:
        for edge in dag.edges():
            f.write("{}, {}\n".format(idx2names[edge[0]], idx2names[edge[1]]))

class Variable:
    def __init__(self, name, r):
        self.name = name
        self.r = r

def compute(infile, outfile):
    df = pd.read_csv(infile, delimiter=',')
    df_max = df.max()
    var_names = list(df.columns)
    vars = [Variable(var_names[i], df_max[i]) for i in range(len(var_names))]
    
    # JUST FOR TESTING
    G = nx.DiGraph()
    for i in range(len(vars)): G.add_node(i)
    for i in range(3): G.add_edge(2*i, 2*i+1)
    #G = k2([i for i in range(len(vars))], vars, df, max_parents=2)

    NUM_ITER_TIMEIT = 1000
    from time import time

    initial_score, init_comp = bayesian_score(vars, G, df)
    print("Initial Bayesian score: {}".format(initial_score))
    print("Initial Bayesian score components: {}".format(init_comp))

    G.add_edge(0, 2)
    t_start = time()
    for _ in range(NUM_ITER_TIMEIT):
        new_score, new_comp = bayesian_score(vars, G, df)
    t_end = time()
    print("\nNew Bayesian score: {}".format(new_score))
    print("New Bayesian score components: {}".format(new_comp))
    print("Time taken for {} iterations: {} s".format(NUM_ITER_TIMEIT, round(t_end - t_start, 2)))

    t_start = time()
    for _ in range(NUM_ITER_TIMEIT):
        clever_score, clever_comp = bayesian_score_recompute_single_var(initial_score, init_comp, vars, G, df, 2)
    t_end = time()
    print("\nClever Bayesian score: {}".format(clever_score))
    print("Clever Bayesian score components: {}".format(clever_comp))
    print("Time taken for {} iterations: {} s".format(NUM_ITER_TIMEIT, round(t_end - t_start, 2)))



# K2 algorithm
def k2(ordering, vars, df, max_parents=2):
    G = nx.DiGraph()
    G.add_nodes_from(list(range(len(ordering))))
    for (k, i) in enumerate(ordering[1:]):
        y = bayesian_score(vars, G, df)
        while True:
            y_best, j_best = -np.inf, 0
            for j in ordering[:k]:
                if not G.has_edge(j, i):
                    G.add_edge(j, i)
                    y_prime = bayesian_score(vars, G, df)
                    if y_prime > y_best:
                        y_best, j_best = y_prime, j
                    G.remove_edge(j, i)
            if y_best > y:
                y = y_best
                G.add_edge(j_best, i)
            else:
                break
    return G



def main():
    if len(sys.argv) != 3:
        raise Exception("usage: python project1.py <infile>.csv <outfile>.gph")

    inputfilename = sys.argv[1]
    outputfilename = sys.argv[2]
    compute(inputfilename, outputfilename)


if __name__ == '__main__':
    main()
